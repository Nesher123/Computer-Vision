{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "##  <span style=\"color:blue\">Exercise 2 - Driver file </span>\n",
    "## <span style=\"color:blue\">Computer Vision - Fall 2020\n",
    "\n",
    "\n",
    "**Lecturer:** Prof. Yael Moses, IDC\n",
    "\n",
    "**TA:** Eyal Friedman, IDC\n",
    "\n",
    "**Sybmission date: 19.12.2020**\n",
    "\n",
    "\n",
    "\n",
    "In this excercise you will practice working with geometric tools for analyzing 3D scenes from 2D images.\n",
    "\n",
    "## Submission guidelines:\n",
    "\n",
    "1. Your zip file should include the following files only:\n",
    "    - ex2-driver.ipynb  **Or**  ex2-driver.py \n",
    "    - ex2_ID_ID.doc  **Or**  ex2_ID_ID.pdf\n",
    "2. The results you are asked to display and the open questions should be answered in a doc/pdf file. \n",
    "   (Don't add the python code to that file.)\n",
    "4. You may use any IDE  (e.g., Spyder, Jupyter Notebook, Pycharm, ect.).\n",
    "5. Name the file 'ex2_ID_ID.zip' and do **not** include any additional directories. \n",
    "6. Submit using *moodle*\n",
    "7. Submit on time!\n",
    "8. You can submit this assignment in pairs (no triples).\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Do not submit your tests, unless requested.\n",
    "3. Use `python 3` and `numpy 1.18.5`. Changes of the configuration we provided are at your own risk. Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works.\n",
    "4. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden, unless been provided by us.\n",
    "4. Your code must run without errors. Note,  **code that fails to  run will not be graded.**\n",
    "5. Document your code properly.\n",
    "\n",
    "## Honor Code:\n",
    "The assignment is a basic tool for learning the material. You can probably find the solution on the Web. However, if  you do so, then you will not learn what you should learn from it. In addition, since we  grade  the assignment, using existing solutions will be considered dishonest.\n",
    "In particular, you are not allowed to copy or use any code that solves the task. \n",
    "You are more than welcome to talk with your friends, but you are not allowed to give your code or answers and you are not allowed to use their code or answers. \n",
    "Remember – you take this course in order to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# This opens an inteactive figure - use it in part B\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "# This specifies the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.7.9\n",
      "Numpy version:  1.18.5\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Numpy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs.google.com/document/d/1nN1hPVZ43l1Xf-8EaNBw7gUjW9M7d_YsBdvx6SyNIfs/edit\n",
    "# Let's write the answers here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Projection\n",
    "\n",
    "**In this part you will go over projection matrix,  and use them to project 3D points to an image.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing values, given partial values of the parameters of the left and right cameras.\n",
    "\n",
    "\\\n",
    "**Right image parameters:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection matrix of the right image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "MR = np.array([[1100.504780,          0,   331.023000,   0],\n",
    "               [0,          1097.763735,   259.386377,   0],\n",
    "               [0,                    0,            1,   0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rotation matrix of the right image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "RR = np.array([[1,0,0],\n",
    "               [0,1,0],\n",
    "               [0,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focal length of the right image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fR = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, replace \"none\" with your answers to the questions. In addition, if there are more than a single possible solution, choose one.\n",
    "Compute the right image center (principal point):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we learned, a camera with rectangular pixels of size 1/sx X 1/sy , with focal length f, and principal point\n",
    "# (O_x, O_y) (i.e., the intersection of the optical axis, Z, with the image plane provided in pixel coordinates)\n",
    "# has the intrinsic calibration matrix:\n",
    "# M_int = [[ alpha_x  0        O_x  0 ]\n",
    "#          [ 0        alpha_y  O_y  0 ]\n",
    "#          [ 0        0        1    0 ]], where alpha_x = s_x * f    and alpha_y = s_y * f\n",
    "\n",
    "# Thereore, we can simply extract the values for o_x and o_y from M, based only on its indices.\n",
    "OxR = MR[0, 2]\n",
    "OyR = MR[1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the right image scale factor which is consistent with MR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As explained in the previous cell's comments, alpha_x = s_x * f    and alpha_y = s_y * f\n",
    "# Thereore, we can simply extract the values for s_x and s_y from M, based only on M's indices and divide by the focal length.\n",
    "SxR = MR[0, 0]/fR\n",
    "SyR = MR[1, 1]/fR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the right image intrinsic matrix which is consistent with MR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.10050478e+03, 0.00000000e+00, 3.31023000e+02],\n",
       "       [0.00000000e+00, 1.09776373e+03, 2.59386377e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mint is a 3 X 4 matrix, which is the part of M without the righnmost column\n",
    "# M = Mint[I|0]\n",
    "MintR = MR[:, 0:3]\n",
    "MintR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Left image parameters**\n",
    " \n",
    "Left image center (principal point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "OxL = 320.798101\n",
    "OyL = 236.431326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SxL = 1095.671499\n",
    "SyL = 1094.559584 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal length of the left image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fL = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation vector w.r.t. the world origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TL = -np.array([[178.2218,18.8171,-13.7744]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotation matrix of the left image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = np.array([[ 0.9891,    0.0602,   -0.1346],\n",
    "               [-0.0590,    0.9982,    0.0134],\n",
    "               [0.1351,   -0.0053,    0.9908]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the intrinsic projection matrix of the left camera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09567150e+03, 0.00000000e+00, 3.20798101e+02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.09455958e+03, 2.36431326e+02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MintL = np.array([[1095.671499,           0,   320.798101,   0],\n",
    "#                   [0,           1094.559584,   236.431326,   0],\n",
    "#                   [0,                     0,            1,   0]])\n",
    "\n",
    "MintL = np.zeros((3,4))\n",
    "MintL[0,0] = SxL * fL\n",
    "MintL[1,1] = SyL * fL\n",
    "MintL[2,2] = 1\n",
    "MintL[0,2] = OxL\n",
    "MintL[1,2] = OyL\n",
    "\n",
    "MintL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the projection matrix of the left camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,4) (3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-56006ce2f66b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# M = M_int[I|0] * M_ext = M_int[I|0] * M_R * M_T\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mML\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMintL\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mRL\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mTL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,4) (3,3) "
     ]
    }
   ],
   "source": [
    "# M = M_int[I|0] * M_ext = M_int[I|0] * M_R * M_T\n",
    "ML = MintL*RL*TL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the COP of the left and the right images, in Cartesian coordinates:   \n",
    "\n",
    "(You may use the the function *null_space* from *scipy.linalg*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL = None\n",
    "CR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distance between CL and CR:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A2: Hands on Triangulation \n",
    "\n",
    "Write a function p = proj(M,P) that recieves as input the 3D point P and a projection matrix M, and outputs the 2D coordinates of the projected point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for proj function\n",
    "def proj(M,P):\n",
    "    \n",
    "    # your code\n",
    "    pass\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tGiven object points in the world coordinate system,  P=(-140,50,1200) and Q=(30,100,2000).\n",
    "\n",
    "    a.\tWhat are the coordinates (Cartesian) of the points in the left camera coordinate system?\\\n",
    "    b.\tWhat are the coordinates (Cartesian) of the points in the right camera coordinate system?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PL = None\n",
    "PR = None\n",
    "QL = None\n",
    "QR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[-140],[50],[1200]])\n",
    "pL = proj(ML,P)\n",
    "pR = proj(MR,P)\n",
    "\n",
    "Q = np.array([[30],[100],[2000]]) \n",
    "qL = proj(ML,Q)\n",
    "qR = proj(MR,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read two images and display the projections of P and Q on the two given images ###\n",
    "\n",
    "[//]: # \" \"\n",
    "The code below should return this result: \n",
    "\n",
    "![Example](PandQprojections1.png \"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2a787976bb9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'col'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Left image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Right image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "imL = cv2.imread('Left.tif', cv2.IMREAD_GRAYSCALE)\n",
    "imR = cv2.imread('Right.tif', cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0) \n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2, sharex='col', sharey='row')\n",
    "\n",
    "ax1.imshow(imL, cmap='gray'), ax1.set_title('Left image'), ax1.scatter(pL[0], pL[1], color='r'), \\\n",
    "    ax1.scatter(qL[0],qL[1], color = 'b')\n",
    "ax2.imshow(imR, cmap='gray'), ax2.set_title('Right image'), ax2.scatter(pR[0], pR[1], color = 'r'), \\\n",
    "    ax2.scatter(qR[0],qR[1], color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question:**\\\n",
    "Look at the projection of each of the points in the two images. One pair looks as expected, and the other doe not. Please give a short explanation of what may have caused it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Epipolar Geometry\n",
    "Compute the fundamental matrix F and the epipoles eL and eR of the left and right images, using their projection matrices.\n",
    "Note, you should normalize F by F(3,3) for improved precision.\n",
    "\n",
    "For the epipoles' computation use the MR and ML and the Center of projections.\n",
    "\n",
    "**Answer Quesion:**\n",
    "Can you double check if they are correct using F? If so, check it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eL = None\n",
    "eR = None\n",
    "F = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epipolar lines ##\n",
    "\n",
    "Click on three different points of the **right** image, and check if the epipolar lines on the left image pass through a pixel that corresponds to the one you picked in the right image. Output the set of epipolar lines overlayed on the pair of  images as shown below.\n",
    "\n",
    "To do so you can use:\n",
    "1. The code below opens the images in a seperate window. You can click on the right image and  capture the click's coordinates by using the function *plt.ginput*.\n",
    "2. Take each point (this can be done by a loop) and calculate its epipolar line  on the left image using F.\n",
    "3. Compute the two endpoints of the line in the image to plot it on the left image. \\\n",
    "    **Hint**: you have linear coefficients - (a,b,c). Calculate the y value in the image for x=0, and x=image.width and plot the result.\\\n",
    "    Use: ax2.plot((x0. xWidth),(yx0, yxWidth))\n",
    "4. Use the set of the points of the right image that you collected, and draw the epipolar lines on the right image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sould open a new figure window outside of jupyter notebook\n",
    "%matplotlib qt  \n",
    "\n",
    "imL = cv2.imread('Left.tif', cv2.IMREAD_GRAYSCALE)\n",
    "imR = cv2.imread('Right.tif', cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0) \n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2, sharex='col', sharey='row')\n",
    "\n",
    "ax1.imshow(imL, cmap='gray'), ax1.set_title('Left image')\n",
    "ax2.imshow(imR, cmap='gray'), ax2.set_title('Right image')\n",
    "\n",
    "data = plt.ginput(3)\n",
    "\n",
    "x_val = [x[0] for x in data]\n",
    "y_val = [x[1] for x in data]\n",
    "\n",
    "ax2.scatter(x_val, y_val, color='r')\n",
    "\n",
    "for x in data: \n",
    "    # Write your own implementation here.\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is what you should see:\n",
    "![Epipolar](epipolarLines1.png \"Epipolar Lines example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C : SIFT and RANSAC/LMedS\n",
    "**Follow the matching to compute F.**\n",
    "\n",
    "https://docs.opencv.org/master/da/de9/tutorial_py_epipolar_geometry.html \n",
    "\n",
    "The example attached here needs some twicks to make it work. First you need to uninstall the opencv package and to install to opencv-contrib package:\n",
    "- pip uninstall opencv-python \n",
    "\n",
    "Then install the contrib version with this:\n",
    "- pip install opencv-contrib-python\n",
    "\n",
    "**<span style=\"color:red\"> Now, you have to use those lines:**\n",
    "- **sift = cv2.xfeatures2d.SIFT_create()**\n",
    "- **kp1, des1 = sift.detectAndCompute(img1, None)**\n",
    "\n",
    "\n",
    "Hereby, we will find the corresponding featues using the SIFT algorithm and match the closet points. The plotted figure showes the best 300 matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imL = cv2.imread('Left.tif', cv2.IMREAD_GRAYSCALE)\n",
    "imR = cv2.imread('Right.tif', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "# In the link above you need to change the next line from cv.SIFT to cv2.xfeatures2d.\n",
    "# Instead of: sift = cv2.SIFT() use:\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(imL,None)\n",
    "kp2, des2 = sift.detectAndCompute(imR,None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)\n",
    "# create FlannBasedMatcher object\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "matching = []\n",
    "# Building a list of points screened by ratio test as per Lowe's paper\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "        pts1.append(kp1[m.queryIdx].pt)\n",
    "        matching.append(m)\n",
    "        \n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matching = sorted(matching, key = lambda x:x.distance)\n",
    "        \n",
    "# Draw first 300 matches.\n",
    "img3 = np.array([])\n",
    "img3 = cv2.drawMatches(imL, kp1, imR, kp2, matching[:300], outImg = img3, flags =2)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0) \n",
    "f, ((ax1)) = plt.subplots(1, 1, sharex='col', sharey='row')\n",
    "ax1.imshow(img3, cmap='gray'), ax1.set_title('Matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not for submission: Look at the obtain results.\n",
    "\n",
    "    a. Do you think all matches are correct?\n",
    "    b. In which regions of the scene, most of the reliable matches were found?\n",
    "    c. Tru the worst 200 mathces as well -- matching[-1-200:]\n",
    "\n",
    "Now, we will use the found matches to compute **F** using *cv2.findFundamentalMat()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)\n",
    "\n",
    "# Computing the F matrix\n",
    "F_calc, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_LMEDS)\n",
    "# We select only inlier points\n",
    "pts1 = pts1[mask.ravel()==1]\n",
    "pts2 = pts2[mask.ravel()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F_calc.T)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets check the computed F_calc:\n",
    "1. Use it to draw the epipolar line as in the example above (change F to F_calc.T)\n",
    "2. Compute the distance between the computed epipoles by F and by F_calc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import null_space\n",
    "e1=null_space(F_calc)\n",
    "e2=null_space(F)\n",
    "\n",
    "e1_c = e1[0:2,0]/e1[2,0]\n",
    "e2_c = e2[0:2,0]/e2[2,0]\n",
    "\n",
    "print(np.linalg.norm(e1_c-e2_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your part in this section :) ###\n",
    "\n",
    "#### Take two images by your camera and compute the epipolar geometry using LMedS ####\n",
    "\n",
    "Please submit at the pdf file your results of the cosine distance and the output of your images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D1 ##\n",
    "\n",
    "**Here you will compute stereo - the 3D structure from a pair of rectified images.**\\\n",
    "Your main program should take two input images (left and right) and a window size. The output of the function should be:\n",
    "\n",
    "a) Computed disparity map for the left image - see instructions below.\n",
    "\n",
    "b) Three matrices X, Y, Z with the x, y, z coordinates of each pixel in the left image. Assign zero for pixels for which the disparity was not computed.\n",
    "\n",
    "Instructions:\n",
    "1. Read the two images view1.png and view5.png.\n",
    "   The image planes are co-planar. The distance between the cameras is 160mm.\n",
    "2. Write an algorithm that receives two rectified images (coplanar and parallel to the line connecting the two COP, with the same focal length), and compute a naive disparity along corresponding epipolar lines:\\\n",
    "    **2.a** For each pair of pixels (one from each image) compute the similarity between  rectangle patches around it using  normalized correlation defined by: $$\\frac{v_1⋅v_2}{‖v_1 ‖‖v_2 ‖}$$ where $v_i$ is a descriptor of the patch.\\\n",
    "    For example, you can use the intenisty and reshape a 3×3 patch to a vector, using the reshape function as follows:\\\n",
    "    ***v_1 = I[i-1:i+1,j-1:j+1].reshape(1,9)***. \\\n",
    "    **2.b.** The correspondence is defined to be the one with the higher similarity. The algorithm should accept as  parameters: the two images, the size of the patch (*s_x,s_y*),  and the disparity range (*d_min,d_max*). For example, if *(d_min,d_max)=(20,120)* it follows that you search for *(x,y)* in the left image, its corresponding patch in the range *(x-120:x-20,y).*\\\n",
    "    **2.c.** The output of this algorithm is a matrix D that consists of the disparity of each pixel.\\\n",
    "    **2.d.** Apply your function to view1.png and view5.png, and display the disparity map as an image. The disparity range is (20,120) (the x location in view1 – the x location in view5).\n",
    "3. To evaluate your result, use back project the points of view1.png to view5.png, and look at the absolute value of their differences. By ‘back project’ we mean to compute view5.png using view1.png by using the computed disparity.\\\n",
    "    **You can use the function** ***dst = cv2.remap(src, map_x, map_y, cv2.INTER_LINEAR)*** where *map_x* and *map_y* are the movements in each axis.\n",
    "    You can use the sum of differences as a score for evaluation. Compute it for 3 different window sizes and present the results in the doc/pdf file.\n",
    "\n",
    "4. Compute the depth map using the disparity. Add to your disparity depth map the value 100, since images were cropped. Note – simple triangulation can be applied here. Display it as an image. Assume that scaled focal lengths (f in the presentation) are α_x=α_y=1.\n",
    "5. Compute the matrices X,Y,Z - and present them as a 3D plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D2 \n",
    "\n",
    "Repeat D1 with using the values of the gradients of the images instead of the intensities for computing the descriptors v_1 and v_2.\n",
    "\n",
    "**Answer Questions:**\n",
    "1. What are the differences in the results of D1 and D2? \n",
    "2. How does the patch size affect the results? \n",
    "3. How does the  order preserving assumption affect the results?\n",
    "4. Which regions have more errors? Why?\n",
    "\n",
    "BONUS: repeat D1 but use dynamic programing to compute an optimal order preserving disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part E ##\n",
    "\n",
    "### Theoretical questions: ###\n",
    "1. Suggest a method to remove outliers when you have three general images of the same scene.\n",
    "2. Compare the following two methods to remove outliers: (a) Use  RANSAC with  the homography model (we will learn it in class 6); (ii) Use  RANSAC with the epipolar geometry (fundamental matrix) model. When do you expect (i) to give better results, and when do you expect (ii)  to give better results? Explain your answer.\n",
    "3. Why is C=nullspace(M)? Go over the proof in Class 4 slides 52-55. It is nice!\\\n",
    "   Answer:\n",
    "   a. Why is the degree of M  at most 3?\n",
    "   b. Let A and B be two 3D points such that B is not on the 3D line that connects A and the center of projection COP.\\\n",
    "      Is it possible that MA=MB? Give a short explanation for your answer.\n",
    "   c. Why is 𝑀(1−𝜆)C=0  (where C is in homogenous coordinates)?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
